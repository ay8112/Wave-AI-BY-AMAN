"""
Wave AI - Simple AI Assistant (v1.0)
Basic version for workshops - clean and easy to understand
Created by Aman singh yadav

"""

from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import google.generativeai as genai
import os
from datetime import datetime
import uvicorn

# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

app = FastAPI(
    title="Wave AI",
    description="Simple AI Assistant for Workshops",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# Initialize Gemini AI with dynamic model discovery
def initialize_ai():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("‚ùå GEMINI_API_KEY not found")
        return None
    
    # Clean and validate API key to prevent header validation errors
    api_key = api_key.strip()
    
    # Basic API key format validation
    if not api_key.startswith("AIza") or len(api_key) != 39:
        print("‚ùå Invalid API key format detected")
        print("üí° API key should start with 'AIza' and be 39 characters long")
        return None
    
    # Check for invalid characters that cause header validation errors
    if not api_key.replace("-", "").replace("_", "").isalnum():
        print("‚ùå API key contains invalid characters")
        return None
    
    try:
        print("üîë Configuring Gemini API...")
        genai.configure(api_key=api_key)
        
        # Discover available models dynamically
        print("üîç Discovering available Gemini models...")
        try:
            available_models = list(genai.list_models())
            if not available_models:
                print("‚ùå No models found in your account")
                return None
                
            print("üìã Available models:")
            for model in available_models[:5]:  # Show first 5 models
                print("   - {}".format(model.name))
            
            # Preferred models in order of preference (stable models first, avoid preview)
            preferred_models = [
                "models/gemini-1.5-flash",
                "models/gemini-1.5-pro", 
                "models/gemini-pro",
                "models/gemini-1.0-pro",
                "models/gemini-2.5-flash",  # New stable model
            ]
            
            # Find the first available preferred model
            selected_model_name = None
            for pref_model in preferred_models:
                for available_model in available_models:
                    if available_model.name == pref_model:
                        if 'generateContent' in available_model.supported_generation_methods:
                            selected_model_name = pref_model
                            break
                if selected_model_name:
                    break
            
            # If no preferred model found, use stable models (avoid preview models)
            if not selected_model_name:
                print("üîç No preferred models found, searching for stable models...")
                for available_model in available_models:
                    if ('generateContent' in available_model.supported_generation_methods and 
                        'preview' not in available_model.name.lower() and
                        'experimental' not in available_model.name.lower()):
                        selected_model_name = available_model.name
                        print("‚úÖ Found stable model: {}".format(selected_model_name))
                        break
            
            # Last resort: use any available model (including preview)
            if not selected_model_name:
                print("‚ö†Ô∏è No stable models found, using first available model...")
                for available_model in available_models:
                    if 'generateContent' in available_model.supported_generation_methods:
                        selected_model_name = available_model.name
                        print("‚ö†Ô∏è Using preview/experimental model: {}".format(selected_model_name))
                        break
            
            if not selected_model_name:
                print("‚ùå No models support generateContent method")
                return None
                
            print("ü§ñ Selected model: {}".format(selected_model_name))
            model = genai.GenerativeModel(selected_model_name)
            
        except Exception as discovery_e:
            print("‚ö†Ô∏è Model discovery failed, trying fallback: {}".format(str(discovery_e)))
            # Fallback to hardcoded model names
            fallback_models = ["gemini-1.5-flash", "gemini-pro", "gemini-1.0-pro"]
            model = None
            for fallback_name in fallback_models:
                try:
                    print("üîÑ Trying fallback model: {}".format(fallback_name))
                    model = genai.GenerativeModel(fallback_name)
                    break
                except:
                    continue
            
            if not model:
                print("‚ùå All fallback models failed")
                return None
        
        # Test the selected model with a safe prompt
        try:
            print("üß™ Testing AI connection...")
            # Use a very safe, neutral test prompt to avoid content filtering
            safe_test_prompt = "Hello, can you say hi?"
            test_response = model.generate_content(safe_test_prompt, generation_config={"max_output_tokens": 20})
            
            if test_response and hasattr(test_response, 'text') and test_response.text:
                print("‚úÖ Wave AI initialized and tested successfully")
                return model
            elif test_response and hasattr(test_response, 'candidates') and test_response.candidates:
                # Check if response was blocked
                candidate = test_response.candidates[0]
                if hasattr(candidate, 'finish_reason'):
                    if candidate.finish_reason == 2:  # RECITATION
                        print("‚ö†Ô∏è Test blocked by content filter (RECITATION), but model should work for normal requests")
                    elif candidate.finish_reason == 3:  # SAFETY
                        print("‚ö†Ô∏è Test blocked by safety filter, but model should work for normal requests")
                    else:
                        print("‚ö†Ô∏è Test had finish_reason: {}".format(candidate.finish_reason))
                return model  # Return model anyway, it might work for actual requests
            else:
                print("‚ö†Ô∏è AI test response was empty, but model should work for actual requests")
                return model
                
        except Exception as test_e:
            error_msg = str(test_e)
            if "finish_reason" in error_msg.lower() or "recitation" in error_msg.lower():
                print("‚ö†Ô∏è Test blocked by content filter, but model should work for normal requests")
            else:
                print("‚ö†Ô∏è AI connection test failed: {}".format(error_msg))
            return model  # Still return model, might work for actual requests
            
    except Exception as e:
        error_msg = str(e).lower()
        if "api" in error_msg and "key" in error_msg:
            print("‚ùå Invalid API key - please check your Gemini API key")
        elif "quota" in error_msg or "limit" in error_msg:
            print("‚ùå API quota exceeded - please check your Gemini API limits")
        else:
            print("‚ùå Failed to initialize AI: {}".format(str(e)))
        return None

# Initialize AI
wave_ai = initialize_ai()

# Startup event to validate everything is ready
@app.on_event("startup")
async def startup_event():
    print("üåä Wave AI container starting...")
    print("üì° Port configured for: {}".format(os.getenv('PORT', '8000')))
    print("üîë API Key configured: {}".format('‚úÖ' if os.getenv('GEMINI_API_KEY') else '‚ùå'))
    if wave_ai:
        print("‚úÖ Wave AI ready to serve requests")
    else:
        print("‚ö†Ô∏è  Wave AI started but AI model may not be available")

# Data Models
class ChatMessage(BaseModel):
    message: str

class ChatResponse(BaseModel):
    response: str
    success: bool = True

# In-memory conversations
conversations = []

# Generate AI Response with timeout protection
async def generate_response(message: str) -> str:
    if not wave_ai:
        return "‚ùå AI is currently offline. Please check configuration."
    
    try:
        # Enhanced generation config with timeout protection  
        prompt = "You are Wave AI, a helpful assistant created by Aman Singh Yadav. Provide clear and helpful responses.\n\nHuman: {}\n\nWave AI:".format(message)
        
        response = wave_ai.generate_content(
            prompt,
            generation_config={
                "max_output_tokens": 1024,
                "temperature": 0.7,
                "top_p": 0.8,
                "top_k": 40,
                "stop_sequences": []
            },
            safety_settings=[
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
            ]
        )
        
        if response and response.text:
            return response.text.strip()
        else:
            return "I couldn't generate a response. Please try rephrasing your question."
        
    except Exception as e:
        error_str = str(e).lower()
        
        # Handle specific error types with user-friendly messages
        if "quota" in error_str or "limit" in error_str:
            return "‚è±Ô∏è I'm experiencing high demand right now. Please try again in a moment."
        elif "timeout" in error_str or "deadline" in error_str:
            return "‚è±Ô∏è Request timed out. Please try again with a shorter message."
        elif "api" in error_str and "key" in error_str:
            return "üîë Configuration issue detected. Please contact support."
        elif "network" in error_str or "connection" in error_str:
            return "üåê Network connectivity issue. Please try again."
        else:
            return "‚ö†Ô∏è I encountered an error while processing your request. Please try again."

            print("üîç AI Generation Error: {}".format(str(e)))  # For debugging
            return "‚ö†Ô∏è I encountered an error while processing you
# Routes
@app.get("/")
async def root():
    return FileResponse("static/index.html")

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "ai_online": wave_ai is not None,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/chat")
async def chat(message: ChatMessage):
    try:
        print("üí¨ Received message: {}...".format(message.message[:50]))
        
        # Generate AI response
        response_text = await generate_response(message.message)
        
        print("‚úÖ Response generated successfully")
        
        # Store conversation
        conversations.append({
            "user": message.message,
            "assistant": response_text,
            "timestamp": datetime.now().isoformat()
        })
        
        return ChatResponse(response=response_text, success=True)
        
    except Exception as e:
        print("‚ùå Chat endpoint error: {}".format(str(e)))
        return ChatResponse(
            response="Sorry, I encountered an unexpected error. Please try again.", 
            success=False
        )

@app.get("/conversations")
async def get_conversations():
    return {"conversations": conversations[-10:]}  # Last 10 messages

@app.delete("/conversations") 
async def clear_conversations():
    global conversations
    conversations = []
    return {"message": "Chat cleared"}

# For local development only
# if __name__ == "__main__":
#     port = int(os.getenv("PORT", 8000))
#     print("üåä Starting Wave AI...")
#     uvicorn.run("main:app", host="0.0.0.0", port=port, reload=False)
